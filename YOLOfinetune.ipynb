{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vREGyn1Lx03K",
        "outputId": "bb0e1bf0-db10-4237-9eb2-b53cf395dc5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (4.10.0.84)\n",
            "Requirement already satisfied: ultralytics in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (8.3.205)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (1.15.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (0.24.0+cu128)\n",
            "Requirement already satisfied: psutil in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (7.1.0)\n",
            "Requirement already satisfied: polars in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (1.34.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.2.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.8.0)\n",
            "Requirement already satisfied: polars-runtime-32==1.34.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from polars->ultralytics) (1.34.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cacereje\\appdata\\local\\miniconda3\\envs\\erdos_spring_2025\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b-KsVoCzwOr"
      },
      "source": [
        "Make sure the following folders are correct for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5badEMgw8Vj",
        "outputId": "a6019088-c24e-4148-ab56-20a0dbe22f54"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zofHdXMxSuA",
        "outputId": "f6508f5a-ad5d-47cd-f313-63fab00541a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Emotion Teller Project\n"
          ]
        }
      ],
      "source": [
        "%cd 'drive/MyDrive/Colab Notebooks/Emotion Teller Project'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJ7IbZUz18a"
      },
      "source": [
        "We import the metadata and remove the rows of missing files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RcaPTDQDegOG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "\n",
        "data_root       = 'Data' # Folder where all data sources are stored\n",
        "data_folders    = 'ImageData' # List of folders containing images in .jpg format\n",
        "data_meta       = ['emotic-relabelled.csv',\n",
        "                'hgel-relabelled.csv'] # List of metadata corresponding to previous list of folders in .csv format. In our case we relabelled some of the data, so this is different from original metadata.\n",
        "meta_root       = 'Metadata'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dshTH2f0xYKa",
        "outputId": "3d81187f-7856-42f7-bf07-3f8155320c31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "file_name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "objects",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "original_width",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "original_height",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "emotions",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "34181a39-ae75-49f3-aac9-a937bc2c71fd",
              "rows": [
                [
                  "0",
                  "2w4w14gc7v0zzlo2hr.jpg",
                  "{'bbox': [[53.203433030794265, 17.615104049146684, 33.96738025028607, 46.64979212224452]], 'categories': ['Sad']}",
                  "300",
                  "361",
                  "['Sad']"
                ],
                [
                  "1",
                  "30nuw6i66lmickmbly.jpg",
                  "{'bbox': [[26.798646725660358, 15.553243651943575, 27.84875745667354, 37.671816179988916]], 'categories': ['Sad']}",
                  "500",
                  "489",
                  "['Sad']"
                ],
                [
                  "2",
                  "6ypnlgwpdwm4cxhzyq.jpg",
                  "{'bbox': [[39.807704879496356, 5.299623762884472, 26.026564641918654, 20.851271193442468]], 'categories': ['Disgust']}",
                  "393",
                  "233",
                  "['Disgust']"
                ],
                [
                  "3",
                  "7yfw4gdcdak0kdvqu2.jpg",
                  "{'bbox': [[47.60784313725491, 6.438353405085614, 25.411764705882423, 49.96970297929128]], 'categories': ['Sad']}",
                  "184",
                  "274",
                  "['Sad']"
                ],
                [
                  "4",
                  "9uq1l58tusi8vij6ju.jpg",
                  "{'bbox': [[38.745098039215634, 17.137253719272767, 22.196078431372474, 48.90196205992708]], 'categories': ['Sad']}",
                  "200",
                  "300",
                  "['Sad']"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>objects</th>\n",
              "      <th>original_width</th>\n",
              "      <th>original_height</th>\n",
              "      <th>emotions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2w4w14gc7v0zzlo2hr.jpg</td>\n",
              "      <td>{'bbox': [[53.203433030794265, 17.615104049146...</td>\n",
              "      <td>300</td>\n",
              "      <td>361</td>\n",
              "      <td>['Sad']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30nuw6i66lmickmbly.jpg</td>\n",
              "      <td>{'bbox': [[26.798646725660358, 15.553243651943...</td>\n",
              "      <td>500</td>\n",
              "      <td>489</td>\n",
              "      <td>['Sad']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6ypnlgwpdwm4cxhzyq.jpg</td>\n",
              "      <td>{'bbox': [[39.807704879496356, 5.2996237628844...</td>\n",
              "      <td>393</td>\n",
              "      <td>233</td>\n",
              "      <td>['Disgust']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7yfw4gdcdak0kdvqu2.jpg</td>\n",
              "      <td>{'bbox': [[47.60784313725491, 6.43835340508561...</td>\n",
              "      <td>184</td>\n",
              "      <td>274</td>\n",
              "      <td>['Sad']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9uq1l58tusi8vij6ju.jpg</td>\n",
              "      <td>{'bbox': [[38.745098039215634, 17.137253719272...</td>\n",
              "      <td>200</td>\n",
              "      <td>300</td>\n",
              "      <td>['Sad']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                file_name                                            objects  \\\n",
              "0  2w4w14gc7v0zzlo2hr.jpg  {'bbox': [[53.203433030794265, 17.615104049146...   \n",
              "1  30nuw6i66lmickmbly.jpg  {'bbox': [[26.798646725660358, 15.553243651943...   \n",
              "2  6ypnlgwpdwm4cxhzyq.jpg  {'bbox': [[39.807704879496356, 5.2996237628844...   \n",
              "3  7yfw4gdcdak0kdvqu2.jpg  {'bbox': [[47.60784313725491, 6.43835340508561...   \n",
              "4  9uq1l58tusi8vij6ju.jpg  {'bbox': [[38.745098039215634, 17.137253719272...   \n",
              "\n",
              "   original_width  original_height     emotions  \n",
              "0             300              361      ['Sad']  \n",
              "1             500              489      ['Sad']  \n",
              "2             393              233  ['Disgust']  \n",
              "3             184              274      ['Sad']  \n",
              "4             200              300      ['Sad']  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(os.path.join(meta_root,'train_meta.csv'))\n",
        "df['objects'] = df['objects'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Check if the file exists for each row\n",
        "df['file_exists'] = df.apply(lambda x: os.path.exists(os.path.join(data_root,data_folders,x.file_name)),axis = 1)\n",
        "\n",
        "# Drop rows where the file does not exist\n",
        "df = df[df.file_exists]\n",
        "df.drop(columns=['file_exists'], inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI1JnG-Nx9V3",
        "outputId": "ef3671cb-772f-484c-eb08-acebc6f3c878"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTtcXahYz5uI"
      },
      "source": [
        "Basic train test split, with no regards for balancing or anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M3BzK7WCel6X"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZFf2kOuizPsr"
      },
      "outputs": [],
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "\n",
        "df['emotions'] = df['objects'].apply(lambda x: x['categories'])\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "X = df[['file_name']]\n",
        "y = mlb.fit_transform(df['emotions'])\n",
        "\n",
        "mss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "idx = np.arange(len(df))\n",
        "\n",
        "(train_idx, test_idx), = mss.split(idx, y)\n",
        "\n",
        "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df  = df.iloc[test_idx].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV9Ueao60BQt"
      },
      "source": [
        "The emotions to be detected by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w4lsEmA-7SLT"
      },
      "outputs": [],
      "source": [
        "emo_dic = {'Neutral':0,'Happy':1,'Surprise':2,'Sad':3,'Angry':4,'Fear':5,'Disgust':6}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K44XYFYp0HUP"
      },
      "source": [
        "The following two cells, create new folders with labels and pictures for our training and validation sets according the structure required by `YOLO`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A6_9ogBY2nGt"
      },
      "outputs": [],
      "source": [
        "yolo_dir = 'YOLO_training'\n",
        "os.makedirs(yolo_dir, exist_ok=True)\n",
        "\n",
        "source = 'Human-Group-Emotions-Labelled/'\n",
        "\n",
        "def xywh2xcycwh(x,y,w,h):\n",
        "  return [x + w/2, y + h/2, w/2, h/2]\n",
        "\n",
        "def label(item):\n",
        "  objs = item['objects']\n",
        "  original_width = item['original_width']\n",
        "  original_height = item['original_height']\n",
        "  text_label = ''\n",
        "  for ind, emotion in enumerate(objs['categories']):\n",
        "    x,y,w,h = np.array(objs['bbox'][ind]) # Unpack the list into four variables\n",
        "    x,y,w,h = x/original_width, y/original_height, w/original_width, h/original_height\n",
        "    xc,yc,wc,hc = xywh2xcycwh(x,y,w,h) # Pass the four variables to the function\n",
        "    text_label += f'{emo_dic[emotion]} {xc} {yc} {wc} {hc}\\n'\n",
        "  return text_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "798fc367",
        "outputId": "83786d83-af07-4ad8-9267-e25642741ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated label files and copied images for train and validation sets.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Create directories for train and validation images and labels\n",
        "os.makedirs(os.path.join(yolo_dir,'images','train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(yolo_dir,'labels','train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(yolo_dir,'images','val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(yolo_dir,'labels','val'), exist_ok=True)\n",
        "\n",
        "# Generate labels and copy images for the training set\n",
        "for index, row in train_df.iterrows():\n",
        "    image_name = row['file_name']\n",
        "    label_content = label(row)\n",
        "    with open(os.path.join(yolo_dir,'labels','train',image_name.replace('.jpg', '.txt')), 'w') as f:\n",
        "        f.write(label_content)\n",
        "    shutil.copy(os.path.join(data_root,data_folders,image_name), os.path.join(yolo_dir,'images','train',image_name))\n",
        "\n",
        "# Generate labels and copy images for the validation set\n",
        "for index, row in val_df.iterrows():\n",
        "    image_name = row['file_name']\n",
        "    label_content = label(row) # FIX THIS\n",
        "    with open(os.path.join(yolo_dir,'labels','val',image_name.replace('.jpg', '.txt')), 'w') as f:\n",
        "        f.write(label_content)\n",
        "    shutil.copy(os.path.join(data_root,data_folders,image_name), os.path.join(yolo_dir,'images/val/',image_name))\n",
        "\n",
        "print(\"Generated label files and copied images for train and validation sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NeHZBuD0Vw6"
      },
      "source": [
        "Make sure `data.yaml` is in the `ds_folder`, this contains the structure for the train/val pictures and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j0qBvp1-9l1",
        "outputId": "32d046f3-9b7f-4e85-8dc3-c7a3caf74906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.221 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.205  Python-3.12.8 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4070 SUPER, 12282MiB)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=YOLO_training\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=300, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\Erdos project\\EmotionTeller\\EmotionTeller\\runs\\detect\\train7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    432037  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
            "YOLO11n summary: 181 layers, 2,591,205 parameters, 2,591,189 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 9.23.9 MB/s, size: 85.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Erdos project\\EmotionTeller\\EmotionTeller\\YOLO_training\\labels\\train... 146 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 146/146 385.6it/s 0.4s0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Erdos project\\EmotionTeller\\EmotionTeller\\YOLO_training\\images\\train\\12_Group_Team_Organized_Group_12_Group_Team_Organized_Group_12_310.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Erdos project\\EmotionTeller\\EmotionTeller\\YOLO_training\\labels\\train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 3.22.2 MB/s, size: 56.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Erdos project\\EmotionTeller\\EmotionTeller\\YOLO_training\\labels\\val... 32 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 32/32 237.6it/s 0.1s.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Erdos project\\EmotionTeller\\EmotionTeller\\YOLO_training\\labels\\val.cache\n",
            "Plotting labels to D:\\Erdos project\\EmotionTeller\\EmotionTeller\\runs\\detect\\train7\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mD:\\Erdos project\\EmotionTeller\\EmotionTeller\\runs\\detect\\train7\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/300      5.64G       4.86      86.48      2.695         68       1024: 40% ━━━━╸─────── 4/10 2.9it/s 19.9s<2.1ssWARNING CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\n",
            "\u001b[K      1/300      5.75G      4.525      105.7      2.888         11       1024: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 32.5s.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 0.3it/s 3.2s\n",
            "                   all         32        195          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/300      5.53G      4.909      98.41      2.242          2       1024: 100% ━━━━━━━━━━━━ 10/10 3.9it/s 2.6s.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.3it/s 0.3s\n",
            "                   all         32        195          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/300      5.76G      4.315         49      1.977         14       1024: 100% ━━━━━━━━━━━━ 10/10 4.2it/s 2.4s.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9it/s 0.4s\n",
            "                   all         32        195          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/300      5.76G      4.702      42.27      1.895         74       1024: 30% ━━━╸──────── 3/10 2.2it/s 0.7s<3.2s\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 381, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 204, in __call__\n    data = t(data)\n           ^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 1492, in __call__\n    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1048576 bytes in function 'cv::OutOfMemoryError'\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\engine\\model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:235\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:396\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_batch_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Warmup\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\utils\\tqdm.py:347\u001b[0m, in \u001b[0;36mTQDM.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoneType\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\build.py:75\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1506\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1504\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1541\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data, worker_idx)\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\_utils.py:769\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[1;31merror\u001b[0m: Caught error in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\base.py\", line 381, in __getitem__\n    return self.transforms(self.get_image_and_label(index))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 204, in __call__\n    data = t(data)\n           ^^^^^^^\n  File \"c:\\Users\\cacereje\\AppData\\Local\\miniconda3\\envs\\erdos_spring_2025\\Lib\\site-packages\\ultralytics\\data\\augment.py\", line 1492, in __call__\n    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 1048576 bytes in function 'cv::OutOfMemoryError'\n\n"
          ]
        }
      ],
      "source": [
        "model = YOLO('yolo11n.pt')\n",
        "\n",
        "results = model.train(data = os.path.join(yolo_dir,'data.yaml'), imgsz=1024, epochs = 300,device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhEFrDI4v5v6"
      },
      "source": [
        "We use our model to predict on train and validation sets. Make sure you select the correct weights for the latest run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taigJMGKMZHX",
        "outputId": "5ceb887e-8e27-490d-fd5a-c45a2ba159ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 1024x1024 3 Neutrals, 1.8ms\n",
            "1: 1024x1024 5 Happys, 1.8ms\n",
            "2: 1024x1024 1 Neutral, 2 Happys, 1.8ms\n",
            "3: 1024x1024 5 Happys, 1.8ms\n",
            "4: 1024x1024 6 Neutrals, 34 Happys, 1.8ms\n",
            "5: 1024x1024 3 Happys, 1.8ms\n",
            "6: 1024x1024 6 Neutrals, 22 Happys, 1.8ms\n",
            "7: 1024x1024 1 Surprise, 1.8ms\n",
            "8: 1024x1024 1 Neutral, 1.8ms\n",
            "9: 1024x1024 1 Neutral, 4 Happys, 1.8ms\n",
            "10: 1024x1024 5 Neutrals, 1.8ms\n",
            "11: 1024x1024 21 Happys, 1.8ms\n",
            "12: 1024x1024 10 Neutrals, 4 Happys, 1.8ms\n",
            "13: 1024x1024 9 Happys, 1.8ms\n",
            "14: 1024x1024 3 Neutrals, 7 Happys, 1.8ms\n",
            "15: 1024x1024 10 Neutrals, 1.8ms\n",
            "16: 1024x1024 3 Happys, 1.8ms\n",
            "17: 1024x1024 2 Neutrals, 1.8ms\n",
            "18: 1024x1024 4 Neutrals, 6 Happys, 1.8ms\n",
            "19: 1024x1024 6 Neutrals, 2 Happys, 3 Surprises, 1.8ms\n",
            "20: 1024x1024 11 Neutrals, 7 Happys, 1.8ms\n",
            "21: 1024x1024 2 Neutrals, 1 Happy, 1.8ms\n",
            "22: 1024x1024 1 Happy, 1.8ms\n",
            "23: 1024x1024 4 Neutrals, 1.8ms\n",
            "24: 1024x1024 1 Neutral, 1.8ms\n",
            "25: 1024x1024 9 Neutrals, 1.8ms\n",
            "26: 1024x1024 3 Neutrals, 1 Happy, 1.8ms\n",
            "27: 1024x1024 1 Happy, 1.8ms\n",
            "28: 1024x1024 4 Neutrals, 2 Happys, 1.8ms\n",
            "29: 1024x1024 4 Happys, 1.8ms\n",
            "30: 1024x1024 4 Neutrals, 1 Happy, 1.8ms\n",
            "31: 1024x1024 1 Happy, 1 Sad, 1.8ms\n",
            "32: 1024x1024 3 Happys, 1.8ms\n",
            "33: 1024x1024 6 Neutrals, 5 Happys, 1 Surprise, 1.8ms\n",
            "34: 1024x1024 1 Happy, 1 Surprise, 1.8ms\n",
            "35: 1024x1024 4 Neutrals, 1 Happy, 1.8ms\n",
            "36: 1024x1024 2 Neutrals, 1 Happy, 1.8ms\n",
            "37: 1024x1024 6 Happys, 1.8ms\n",
            "38: 1024x1024 1 Neutral, 1.8ms\n",
            "39: 1024x1024 8 Neutrals, 1.8ms\n",
            "40: 1024x1024 5 Neutrals, 1 Happy, 1.8ms\n",
            "41: 1024x1024 2 Neutrals, 1 Happy, 1.8ms\n",
            "42: 1024x1024 1 Neutral, 1 Happy, 1.8ms\n",
            "43: 1024x1024 9 Happys, 1.8ms\n",
            "44: 1024x1024 6 Neutrals, 1.8ms\n",
            "45: 1024x1024 1 Neutral, 1.8ms\n",
            "46: 1024x1024 4 Neutrals, 1.8ms\n",
            "47: 1024x1024 1 Neutral, 3 Happys, 1.8ms\n",
            "48: 1024x1024 12 Neutrals, 1 Happy, 1.8ms\n",
            "49: 1024x1024 (no detections), 1.8ms\n",
            "50: 1024x1024 4 Neutrals, 1 Happy, 1.8ms\n",
            "51: 1024x1024 1 Neutral, 1 Surprise, 1.8ms\n",
            "52: 1024x1024 2 Happys, 1.8ms\n",
            "53: 1024x1024 1 Surprise, 1.8ms\n",
            "54: 1024x1024 9 Surprises, 1.8ms\n",
            "55: 1024x1024 1 Neutral, 5 Happys, 1.8ms\n",
            "56: 1024x1024 1 Neutral, 1 Happy, 1.8ms\n",
            "57: 1024x1024 6 Neutrals, 1 Happy, 1.8ms\n",
            "58: 1024x1024 5 Neutrals, 1 Happy, 1.8ms\n",
            "59: 1024x1024 8 Neutrals, 1.8ms\n",
            "60: 1024x1024 3 Neutrals, 1 Happy, 1.8ms\n",
            "61: 1024x1024 1 Neutral, 42 Happys, 1.8ms\n",
            "62: 1024x1024 5 Neutrals, 2 Happys, 1.8ms\n",
            "63: 1024x1024 1 Neutral, 1.8ms\n",
            "64: 1024x1024 1 Happy, 1 Surprise, 1.8ms\n",
            "65: 1024x1024 1 Happy, 1.8ms\n",
            "66: 1024x1024 4 Happys, 1.8ms\n",
            "67: 1024x1024 2 Happys, 1.8ms\n",
            "68: 1024x1024 1 Neutral, 1.8ms\n",
            "69: 1024x1024 6 Happys, 1.8ms\n",
            "70: 1024x1024 1 Neutral, 1.8ms\n",
            "71: 1024x1024 6 Neutrals, 20 Happys, 1.8ms\n",
            "72: 1024x1024 10 Happys, 1.8ms\n",
            "73: 1024x1024 4 Happys, 1.8ms\n",
            "74: 1024x1024 6 Happys, 1.8ms\n",
            "75: 1024x1024 4 Neutrals, 1.8ms\n",
            "76: 1024x1024 4 Neutrals, 2 Happys, 1.8ms\n",
            "77: 1024x1024 1 Neutral, 1.8ms\n",
            "78: 1024x1024 27 Neutrals, 1.8ms\n",
            "79: 1024x1024 3 Neutrals, 6 Happys, 1.8ms\n",
            "80: 1024x1024 1 Neutral, 11 Happys, 1.8ms\n",
            "81: 1024x1024 (no detections), 1.8ms\n",
            "82: 1024x1024 3 Neutrals, 18 Happys, 1.8ms\n",
            "83: 1024x1024 7 Neutrals, 2 Happys, 1.8ms\n",
            "84: 1024x1024 1 Happy, 1.8ms\n",
            "85: 1024x1024 1 Surprise, 1.8ms\n",
            "86: 1024x1024 5 Neutrals, 1.8ms\n",
            "87: 1024x1024 2 Neutrals, 1.8ms\n",
            "88: 1024x1024 2 Neutrals, 4 Happys, 1.8ms\n",
            "89: 1024x1024 1 Neutral, 1 Happy, 1.8ms\n",
            "90: 1024x1024 4 Neutrals, 1.8ms\n",
            "91: 1024x1024 6 Neutrals, 1.8ms\n",
            "92: 1024x1024 1 Neutral, 1 Happy, 1.8ms\n",
            "93: 1024x1024 2 Happys, 1.8ms\n",
            "94: 1024x1024 5 Neutrals, 3 Happys, 1.8ms\n",
            "95: 1024x1024 7 Neutrals, 2 Happys, 1.8ms\n",
            "96: 1024x1024 3 Neutrals, 12 Happys, 1.8ms\n",
            "97: 1024x1024 5 Neutrals, 1.8ms\n",
            "98: 1024x1024 6 Neutrals, 1.8ms\n",
            "99: 1024x1024 1 Neutral, 2 Happys, 1.8ms\n",
            "100: 1024x1024 6 Happys, 1.8ms\n",
            "101: 1024x1024 4 Neutrals, 1.8ms\n",
            "102: 1024x1024 11 Happys, 1 Surprise, 1.8ms\n",
            "103: 1024x1024 2 Neutrals, 1 Happy, 1.8ms\n",
            "104: 1024x1024 1 Happy, 1.8ms\n",
            "105: 1024x1024 5 Neutrals, 2 Happys, 1.8ms\n",
            "106: 1024x1024 3 Neutrals, 1.8ms\n",
            "107: 1024x1024 1 Neutral, 1.8ms\n",
            "108: 1024x1024 2 Neutrals, 1.8ms\n",
            "109: 1024x1024 4 Happys, 1.8ms\n",
            "110: 1024x1024 4 Happys, 1.8ms\n",
            "111: 1024x1024 2 Neutrals, 7 Happys, 1.8ms\n",
            "112: 1024x1024 2 Neutrals, 1 Happy, 1 Surprise, 1.8ms\n",
            "113: 1024x1024 3 Happys, 1.8ms\n",
            "114: 1024x1024 1 Happy, 1.8ms\n",
            "115: 1024x1024 2 Neutrals, 1.8ms\n",
            "116: 1024x1024 3 Neutrals, 5 Happys, 1.8ms\n",
            "117: 1024x1024 1 Happy, 1.8ms\n",
            "118: 1024x1024 (no detections), 1.8ms\n",
            "119: 1024x1024 5 Neutrals, 1.8ms\n",
            "120: 1024x1024 4 Neutrals, 1.8ms\n",
            "121: 1024x1024 3 Neutrals, 9 Happys, 1.8ms\n",
            "122: 1024x1024 1 Neutral, 1.8ms\n",
            "123: 1024x1024 1 Surprise, 1 Sad, 1.8ms\n",
            "124: 1024x1024 1 Happy, 1.8ms\n",
            "125: 1024x1024 2 Neutrals, 2 Happys, 1.8ms\n",
            "126: 1024x1024 1 Neutral, 1 Surprise, 1.8ms\n",
            "127: 1024x1024 2 Neutrals, 2 Happys, 1.8ms\n",
            "128: 1024x1024 1 Neutral, 1 Happy, 1.8ms\n",
            "Speed: 6.1ms preprocess, 1.8ms inference, 5.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "\n",
            "0: 1024x1024 8 Neutrals, 3.3ms\n",
            "1: 1024x1024 2 Happys, 3.3ms\n",
            "2: 1024x1024 10 Neutrals, 7 Happys, 3.3ms\n",
            "3: 1024x1024 1 Neutral, 1 Happy, 3.3ms\n",
            "4: 1024x1024 1 Happy, 3.3ms\n",
            "5: 1024x1024 1 Happy, 1 Surprise, 3.3ms\n",
            "6: 1024x1024 (no detections), 3.3ms\n",
            "7: 1024x1024 6 Neutrals, 1 Happy, 3.3ms\n",
            "8: 1024x1024 11 Neutrals, 3.3ms\n",
            "9: 1024x1024 (no detections), 3.3ms\n",
            "10: 1024x1024 1 Happy, 3.3ms\n",
            "11: 1024x1024 1 Neutral, 3 Happys, 1 Surprise, 3.3ms\n",
            "12: 1024x1024 1 Happy, 3.3ms\n",
            "13: 1024x1024 4 Neutrals, 2 Happys, 3.3ms\n",
            "14: 1024x1024 (no detections), 3.3ms\n",
            "15: 1024x1024 (no detections), 3.3ms\n",
            "16: 1024x1024 2 Neutrals, 2 Happys, 3.3ms\n",
            "17: 1024x1024 1 Neutral, 2 Happys, 3.3ms\n",
            "18: 1024x1024 5 Neutrals, 1 Happy, 3.3ms\n",
            "19: 1024x1024 1 Happy, 3.3ms\n",
            "20: 1024x1024 5 Neutrals, 3.3ms\n",
            "21: 1024x1024 (no detections), 3.3ms\n",
            "22: 1024x1024 1 Neutral, 1 Happy, 3.3ms\n",
            "23: 1024x1024 5 Neutrals, 2 Happys, 3.3ms\n",
            "24: 1024x1024 2 Happys, 3.3ms\n",
            "25: 1024x1024 1 Neutral, 4 Happys, 3.3ms\n",
            "26: 1024x1024 4 Neutrals, 1 Happy, 3.3ms\n",
            "27: 1024x1024 3 Happys, 3.3ms\n",
            "28: 1024x1024 (no detections), 3.3ms\n",
            "29: 1024x1024 (no detections), 3.3ms\n",
            "30: 1024x1024 8 Neutrals, 3 Happys, 3.3ms\n",
            "31: 1024x1024 5 Neutrals, 10 Happys, 3.3ms\n",
            "32: 1024x1024 8 Happys, 3.3ms\n",
            "Speed: 5.1ms preprocess, 3.3ms inference, 2.2ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        }
      ],
      "source": [
        "fine_tuned = YOLO('runs/detect/train4/weights/best.pt')\n",
        "\n",
        "val_df['path_to_img'] = path+'images/val/'+val_df['file_name']\n",
        "\n",
        "train_df['path_to_img'] = path+'images/train/'+train_df['file_name']\n",
        "\n",
        "train_preds = fine_tuned.predict(list(train_df['path_to_img']))\n",
        "val_preds = fine_tuned.predict(list(val_df['path_to_img']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk19cIugyI4M"
      },
      "source": [
        "To see how this compares to original labelling, set `pic_id` to any index in the training dataset and run the cell below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5rB-ghwhySJC"
      },
      "outputs": [],
      "source": [
        "pic_id=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "oGNDolyMvM7h",
        "outputId": "adb5f086-df94-4fef-fe51-f0c55331b4c6"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1911971694.py, line 29)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[33], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    Display the image with model predictions on the right subplot\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "test = train_df.iloc[pic_id]\n",
        "\n",
        "width = test['original_width']\n",
        "height = test['original_height']\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Display the original image with ground truth bounding boxes on the left subplot\n",
        "axes[0].imshow(Image.open(os.path.join(yolo_dir,'images','train',test['file_name'])))\n",
        "axes[0].set_title('Ground Truth')\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "for bbox in test['objects']['bbox']:\n",
        "  bbox = np.array(bbox)/100\n",
        "  x, y, w, h = bbox*[width,height,width,height]\n",
        "\n",
        "  # Create a Rectangle patch\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "\n",
        "  # Add the patch to the Axes\n",
        "  axes[0].add_patch(rect)\n",
        "\n",
        "Display the image with model predictions on the right subplot\n",
        "axes[1].imshow(train_preds[pic_id].plot(conf = False, line_width = int(width*0.003))) # YOLO's plot method returns an image\n",
        "axes[1].set_title('Model Predictions')\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "erdos_spring_2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
